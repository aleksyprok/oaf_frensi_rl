{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4okHE4FoXfiQ"
   },
   "source": [
    "# Applications of Reinforcement Learning in Finance - Deep Q-learning & the S&P500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5SekTJXXfiT"
   },
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKFgy8b6XfiU"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25615,
     "status": "ok",
     "timestamp": 1654870010576,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "1hhpkLzFXfiV",
    "outputId": "de95b92c-6aa7-436a-de25-5c9c5e639dc6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %cd /content/gdrive/MyDrive/\n",
    "\n",
    "from pathlib import Path\n",
    "results_path = Path('result')        # Results directory\n",
    "if not results_path.exists():         # Set up if not existing\n",
    "  results_path.mkdir(parents=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from time import time\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from psutil import virtual_memory\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')\n",
    "\n",
    "  # Use GPU if available\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4soQqr898RiW"
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1654870010579,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "t8r9-TvR8WI2",
    "outputId": "f58896fd-342d-40c0-9ee7-a9ed7bfc0204"
   },
   "outputs": [],
   "source": [
    "model = 3                           # Model for Testing\n",
    "\n",
    "\n",
    "total_steps = 0\n",
    "max_episodes = 50                  # Episodes to train (change to 1000)\n",
    "\n",
    "gamma = 0.90                        # discount factor\n",
    "tau = 100                           # target network update frequency\n",
    "\n",
    "## NN Architecture\n",
    "architecture = (64, 64)             # units per layer (maybe 256)\n",
    "learning_rate = 0.0001              # learning rate\n",
    "l2_reg = 1e-6                       # L2 regularization\n",
    "\n",
    "## Experience Replay\n",
    "replay_capacity = int(1e6)          # Capacity\n",
    "batch_size = 4096                   # for training set to 4096, the batch size to train the NNs on\n",
    "\n",
    "## epsilon-greedy Policy\n",
    "epsilon_start = 1.0                 # epsilon start value\n",
    "epsilon_end = .01                   # where it stops\n",
    "epsilon_decay_steps = 250           # steps to decay\n",
    "epsilon_exponential_decay = .99     # exponential decay multiplicator\n",
    "\n",
    "trading_cost_bps = 1e-4\n",
    "time_cost_bps = 1e-5\n",
    "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.3%}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP3fITvTXfiY"
   },
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3854,
     "status": "ok",
     "timestamp": 1654870014423,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "9ussIycaXfiY",
    "outputId": "de2be1e7-96e2-415b-8360-5581fc1b88c6"
   },
   "outputs": [],
   "source": [
    "## Helper Function+\n",
    "np.random.seed(41)                              # Set Random\n",
    "tf.random.set_seed(41)                          # Set Random\n",
    "\n",
    "sns.set_style('whitegrid')                      # Plot Style\n",
    "\n",
    "def format_time(t):                             # Helper Function\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)\n",
    "\n",
    "## Set up Gym Environment\n",
    "trading_days = 252\n",
    "register(\n",
    "    id='trading-v0',\n",
    "    entry_point = \"trading_env:TradingEnvironment\",\n",
    "    max_episode_steps=trading_days)\n",
    "\n",
    "\n",
    "\n",
    "## Initialize Trading Environment\n",
    "\n",
    "trading_environment = gym.make('trading-v0',\n",
    "                               ticker='SPX',\n",
    "                               trading_days=trading_days,\n",
    "                               trading_cost_bps=trading_cost_bps,\n",
    "                               time_cost_bps=time_cost_bps,\n",
    "                               model = model)\n",
    "trading_environment.seed(41)\n",
    "\n",
    "# state_dim = trading_environment.observation_space                   # Input Nodes\n",
    "# Alex edit\n",
    "state_dim = trading_environment.observation_space.shape[0]\n",
    "num_actions = trading_environment.action_space.n                    # Number of Actions\n",
    "max_episode_steps = trading_environment.spec.max_episode_steps      # Episode Length (252)\n",
    "\n",
    "if state_dim == 2:\n",
    "  print(\"Model 0,\", \"Episodes :\" + str(max_episodes))\n",
    "elif state_dim == 4:\n",
    "  print(\"Model 1,\", \"Episodes :\" + str(max_episodes))\n",
    "elif state_dim == 6:\n",
    "  print(\"Model 2,\", \"Episodes :\" + str(max_episodes))\n",
    "elif state_dim == 8:\n",
    "  print(\"Model 3,\", \"Episodes :\" + str(max_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SpuP24pXfii"
   },
   "source": [
    "## Trading Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsjUsKBA-0u1"
   },
   "source": [
    "### Define Trading Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1654870014425,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "QENRfl4hXfij"
   },
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 architecture,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "\n",
    "        self.state_dim = state_dim                              # Input Nodes\n",
    "        self.num_actions = num_actions                          # Number of Actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)     # Where to store trained data\n",
    "        self.learning_rate = learning_rate                      # DDQN learning rate\n",
    "        self.gamma = gamma                                      # Discountfactor for future Rewards\n",
    "        self.architecture = architecture                        # Architecture of the DDQN\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.online_network = self.build_model()                # Trainable Network\n",
    "        self.target_network = self.build_model(trainable=False) # Target Network (Second Network)\n",
    "        self.update_target()                                    # Update Target Network with weight of Online Network\n",
    "\n",
    "        self.epsilon = epsilon_start                            # Epsilon = 1\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps          # Decay steps of Epsilon\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps    # How much to decay.\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []                               # Store epsilon values\n",
    "\n",
    "        self.total_steps = self.train_steps = 0                         # Number of steps taken\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0   # Number of episodes\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size        # Number of batches for DDQN to train\n",
    "        self.tau = tau                      # Steps until the update of Target Network\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "    # Creation of Online Network with Keras (Architecture, etc.)\n",
    "    def build_model(self, trainable=True):\n",
    "        layers = []\n",
    "        n = len(self.architecture)\n",
    "        for i, units in enumerate(self.architecture, 1):\n",
    "            # Dense Layer No.1\n",
    "            layers.append(Dense(units=units,\n",
    "                                # state_dim as input nodes\n",
    "                                input_dim=self.state_dim if i == 1 else None,\n",
    "                                activation='relu',  # Rectified Linear Unit\n",
    "                                kernel_regularizer=l2(self.l2_reg),\n",
    "                                name=f'Dense_{i}',\n",
    "                                trainable=trainable))\n",
    "            # Dropout Layer\n",
    "        layers.append(Dropout(.1))\n",
    "            # Dense Layer No.2\n",
    "        layers.append(Dense(units=self.num_actions,\n",
    "                            trainable=trainable,\n",
    "                            name='Output'))\n",
    "\n",
    "        model = Sequential(layers)                              # Initialize the model the model\n",
    "        model.compile(loss='mean_squared_error',                # Compile the model\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # Update Target Network with weights of Online Network\n",
    "    def update_target(self):\n",
    "        self.target_network.set_weights(self.online_network.get_weights())\n",
    "\n",
    "    # Epsilon-Greedy Policy: Either Random action or based on Online Network prediciton.\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps += 1\n",
    "        # Random Number below Epsilon take random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        # Take action based on Online Network prediciton with highest Q-value\n",
    "        q = self.online_network.predict(state)\n",
    "        return np.argmax(q, axis=1).squeeze()\n",
    "\n",
    "    # Stores state, action, reward, next_state\n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        # Check if 252 day are over\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        # If 1 Episode is over.\n",
    "        else:\n",
    "            if self.train:\n",
    "                # Decrease epsilon incrementally or exponentially.\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            # Append new data/numbers\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "        # Store as experience\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "    # Experience Replay method for training the DDQN\n",
    "    def experience_replay(self):\n",
    "        # Start Experience Replay when experience is longer than batch_size (4096)\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        # Minibatch of data\n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "        # Predicted Q-Value for next state and take the best one.\n",
    "        next_q_values = self.online_network.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        # Predicted Q-Value from Target Network.\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target,\n",
    "                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        # Expected Q-Value the target with reward and discount factor.\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        # Predicted Q-Value and store the new target Q-Value\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        # q_values[[self.idx, actions]] = targets\n",
    "        # Alex edit\n",
    "        # print(np.shape(q_values))\n",
    "        # print(np.shape(self.idx))\n",
    "        # print(np.shape(actions))\n",
    "        # print(np.shape(targets))\n",
    "        q_values[self.idx, actions] = targets\n",
    "\n",
    "        # Train network with (s', a') and store loss\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        # If tau steps are reached update the target\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9PBbINVXfim"
   },
   "source": [
    "### Create DDQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4937,
     "status": "ok",
     "timestamp": 1654870019356,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "XPlx92nrXfio",
    "outputId": "a45e05a3-e3d9-4552-a610-ea953d3d4655"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "ddqn = DDQNAgent(state_dim=state_dim,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 architecture=architecture,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "## Store Result Function\n",
    "\n",
    "def save_training(results_path, max_episodes, navs, market_navs, diffs):\n",
    "        training_results = pd.DataFrame({'Episode': list(range(1, len(navs)+1)),\n",
    "                                         'Agent Ep NAV': navs,\n",
    "                                         'Market Ep NAV': market_navs,\n",
    "                                         'Agent CumSum NAV': np.cumsum(navs),\n",
    "                                         'Market CumSum NAV': np.cumsum(market_navs),\n",
    "                                         'Difference': diffs}).set_index('Episode')\n",
    "        training_results['Strategy Wins (%)'] = (training_results.Difference > 0).rolling(100).sum() # Win in % of last 100 episodes\n",
    "        training_results.to_csv(results_path / 'training_results.csv', index=True)\n",
    "        print(training_results.info())\n",
    "\n",
    "\n",
    "def save_test(results_path, test_date, test_action, ret_agent, ret_market):\n",
    "    test_results = pd.DataFrame({'Date': test_date,\n",
    "                                 'Action': test_action,\n",
    "                                 'Agent Return': ret_agent,\n",
    "                                 'Market Return': ret_market,\n",
    "                                 \"Agent NAV\": np.cumsum(ret_agent),\n",
    "                                 \"Market NAV\": np.cumsum(ret_market)}).set_index('Date')\n",
    "    test_results.to_csv(results_path / 'test_results.csv', index=True)\n",
    "    print(test_results.info())\n",
    "\n",
    "## Test model Function\n",
    "\n",
    "def testing_model(ddqn, test_len):\n",
    "    ddqn_test = ddqn\n",
    "\n",
    "    test_date = []\n",
    "    test_action = []\n",
    "    ret_agent = []\n",
    "    ret_market = []\n",
    "    for episode in range(1, test_len+1):\n",
    "        this_state = trading_environment.reset(training=False)            # S_t = [R1, R5] (Reset the Environment) training = False\n",
    "\n",
    "        for episode_step in range(max_episode_steps):       # [0,....,251]\n",
    "            action = ddqn_test.epsilon_greedy_policy(this_state.reshape(-1, state_dim))  # A_t = [0, 1, 2] (Gives Action)\n",
    "\n",
    "            # Execute Action\n",
    "            # next_state, reward, done, _ = trading_environment.step(action)      # S_t+1, R_t, done\n",
    "            # Alex edit\n",
    "            next_state, reward, done, _, _ = trading_environment.step(action)\n",
    "\n",
    "\n",
    "            # Performance Data\n",
    "            test_action.append(int(action))\n",
    "            test_date.append(trading_environment.data_source.date)\n",
    "            ret_agent.append(reward)\n",
    "            ret_market.append(trading_environment.data_source.target)\n",
    "\n",
    "            # Store Results\n",
    "            ddqn_test.memorize_transition(this_state,\n",
    "                                          action,\n",
    "                                          reward,\n",
    "                                          next_state,\n",
    "                                          0.0 if done else 1.0) # If episode is over.\n",
    "            # Train Network\n",
    "            if ddqn_test.train:\n",
    "                ddqn_test.experience_replay()\n",
    "            if done:\n",
    "                break\n",
    "            # New state for next sequence\n",
    "            this_state = next_state\n",
    "    trading_environment.close()\n",
    "    return test_date, test_action, ret_agent, ret_market\n",
    "\n",
    "## Visualization\n",
    "\n",
    "def track_results(episode, total,\n",
    "                  nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, epsilon):\n",
    "\n",
    "    time_ma = np.mean([episode_time[-100:]])                            # Currently no meaning\n",
    "    T = np.sum(episode_time)                                            # Currently no meaning\n",
    "\n",
    "    ## Outputs while training\n",
    "    pretext = \"Training Results\"\n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    # Number of episode, time, Moving Average of last 100 / 10 obs, win_ratio of trading agent, epsilon value.\n",
    "    print(pretext + template.format(episode, format_time(total),\n",
    "                                    nav_ma_100, nav_ma_10,\n",
    "                                    market_nav_100, market_nav_10,\n",
    "                                    win_ratio, epsilon))\n",
    "\n",
    "ddqn.online_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-OJMxB6Xfip"
   },
   "source": [
    "## Train and Test Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 8416,
     "status": "error",
     "timestamp": 1654870028415,
     "user": {
      "displayName": "Frensi Zejnullahu",
      "userId": "02346894623647773197"
     },
     "user_tz": -120
    },
    "id": "G-jwHohcXfiu",
    "outputId": "e4dfc05e-ca0b-4819-8925-2eb48992fcd5"
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "results = []\n",
    "## Initialize Variables\n",
    "episode_time, navs, market_navs, diffs = [], [], [], []\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"model: \", model)\n",
    "print(\"trading_cost_bps: \", trading_cost_bps)\n",
    "print(\"time_cost_bps: \", time_cost_bps)\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"max_episodes: \", max_episodes)\n",
    "print(\"epsilon_decay_steps: \", epsilon_decay_steps)\n",
    "print(\"-----------------------------------------------\")\n",
    "for episode in range(1, max_episodes + 1):\n",
    "\n",
    "    this_state = trading_environment.reset()            # S_t = [R1, R5] (Reset the Environment)\n",
    "    for episode_step in range(max_episode_steps):       # [0,....,251]\n",
    "        # A_t = [0, 1, 2] (Gives Action)\n",
    "        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n",
    "        # Execute Action\n",
    "        # next_state, reward, done, _ = trading_environment.step(action)      # S_t+1, R_t, done\n",
    "        # alex edit\n",
    "        next_state, reward, done, _, _ = trading_environment.step(action)\n",
    "\n",
    "        # Store Results\n",
    "        ddqn.memorize_transition(this_state,\n",
    "                                 action,\n",
    "                                 reward,\n",
    "                                 next_state,\n",
    "                                 0.0 if done else 1.0) # If episode is over.\n",
    "        # Train Network\n",
    "        if ddqn.train:\n",
    "            ddqn.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        # New state for next sequence\n",
    "        this_state = next_state\n",
    "\n",
    "    # get DataFrame with sequence of actions, returns and nav values\n",
    "    result = trading_environment.env.simulator.result()\n",
    "\n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # NAV of last step\n",
    "    nav = final.nav\n",
    "    navs.append(nav)\n",
    "    # market nav\n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    # track difference between agent and market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "\n",
    "    # For MA(100) --> need more than 100 episodes\n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode,\n",
    "                      time() - start,\n",
    "                      # show mov. average results for 100 (10) episodes\n",
    "                      np.mean(navs[-10:]),                 # Last 100 episodes\n",
    "                      np.mean(navs[-5:]),                  # Last 10 episodes\n",
    "                      np.mean(market_navs[-10:]),          # Last 100 episodes\n",
    "                      np.mean(market_navs[-5:]),           # Last 10 episodes\n",
    "\n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      # sum of all diffs higher than 0 (trading agent better than marktet)\n",
    "                      # from last 100 (or less) observation\n",
    "                      # divided by 100 or less (len of diffs)\n",
    "                      np.sum([s > 0 for s in diffs[-10:]])/min(len(diffs), 10),\n",
    "                      ddqn.epsilon) # epsilon value\n",
    "\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "print(\"Final Stats:\")\n",
    "# Same as described above\n",
    "track_results(episode,\n",
    "                      time() - start,\n",
    "                      np.mean(navs[-100:]),                 # Last 100 episodes\n",
    "                      np.mean(navs[-10:]),                  # Last 10 episodes\n",
    "                      np.mean(market_navs[-100:]),          # Last 100 episodes\n",
    "                      np.mean(market_navs[-10:]),           # Last 10 episodes\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100),\n",
    "                      ddqn.epsilon) # epsilon value\n",
    "\n",
    "#### Test the trained model\n",
    "\n",
    "if trading_environment.data_source.counter != 0:        # Check the test counter\n",
    "    trading_environment.data_source.counter = 0\n",
    "\n",
    "test_len = int(np.floor(len(trading_environment.data_source.y_test.index)/trading_days))    # How many years of terst data we got.\n",
    "\n",
    "if test_len < 1:\n",
    "    print(\"Your Test Data is too short.\")           # If less than one year will raise error.\n",
    "\n",
    "test_date, test_action, ret_agent, ret_market = testing_model(ddqn=ddqn, test_len=test_len) # Testing model\n",
    "\n",
    "trading_environment.close()\n",
    "\n",
    "save_training(results_path=results_path, max_episodes=max_episodes,\n",
    "              navs=navs, market_navs=market_navs, diffs=diffs)\n",
    "save_test(results_path=results_path,\n",
    "          test_date=test_date, test_action=test_action, ret_agent=ret_agent, ret_market=ret_market)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPfjNvPR8BrL"
   },
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BT1w1lF3Xfiv"
   },
   "outputs": [],
   "source": [
    "test_res = pd.read_csv(str(results_path) + \"/test_results.csv\", index_col = \"Date\")\n",
    "test_res.Action = test_res.Action - 1\n",
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htu3NfXJXfiw"
   },
   "outputs": [],
   "source": [
    "test_res[[\"Agent NAV\",\"Market NAV\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIIO9e2iy3Jo"
   },
   "outputs": [],
   "source": [
    "train_res = pd.read_csv(str(results_path) + \"/training_results.csv\", index_col = \"Episode\")\n",
    "train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOctQz3oXfiy"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "df1 = (train_res[['Agent Ep NAV', 'Market Ep NAV']]\n",
    "       .rolling(100) # was 100\n",
    "       .mean())\n",
    "df1.plot(ax=axes[0],\n",
    "         title='Annual Returns (Moving Average)',\n",
    "         lw=1)\n",
    "\n",
    "df2 = train_res['Strategy Wins (%)'].div(100).rolling(50).mean() # was 100 and 50\n",
    "df2.plot(ax=axes[1],\n",
    "         title='Agent Outperformance (%, Moving Average)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.yaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\n",
    "    ax.xaxis.set_major_formatter(\n",
    "        FuncFormatter(lambda x, _: '{:,.0f}'.format(x)))\n",
    "axes[1].axhline(.5, ls='--', c='k', lw=1)\n",
    "\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(results_path / 'performance', dpi=300)                     # Save the pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYoWS6waXfiz"
   },
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    sns.distplot(train_res.Difference)\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DwNtSkO1NNI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [
    "RKFgy8b6XfiU",
    "4soQqr898RiW",
    "RP3fITvTXfiY",
    "1SpuP24pXfii",
    "WsjUsKBA-0u1",
    "l-OJMxB6Xfip",
    "fPfjNvPR8BrL"
   ],
   "machine_shape": "hm",
   "name": "Colab_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
